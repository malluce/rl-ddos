tcam_cap=200
HafnerRewardCalc.tcam_cap=%tcam_cap
HafnerObservations.tcam_cap =%tcam_cap
TrainLoop.num_iterations = 20000 # 10k episodes in original setup, fewer in our setup (same number of steps though)
DistributionTrace.traffic_trace_construct = @HafnerT1
Loop.action_interval=10

TrainLoop.collect_raw = False

# generic params
Loop.epsilon=0.001
DQNWrapAgent.lr=1e-3
Loop.sampling_rate=0.2
TrainLoop.gamma=0.95

# epsilon decay
DQNWrapAgent.eps_greedy_decay_exp=True
MinExpSchedule.lr=1.0
MinExpSchedule.lr_decay_steps=1
MinExpSchedule.lr_decay_rate=0.99
MinExpSchedule.min_lr=0.01

# replay buf
TrainLoop.batch_size = 32
TrainLoop.replay_buf_size = 2000
TrainLoop.initial_collect_steps = 32 # Listing 5.5 (start training after batch size is reached)

# hard target net copies, at the end of each episode
DQNWrapAgent.target_update_tau=1
DQNWrapAgent.target_update_period=38

# NN
DQNWrapAgent.q_layers=(24,24)

# no CNN
TrainLoop.image_gen=None

# action
TrainLoop.actionset_selection=@HafnerActionSet()

# state
TrainLoop.use_prev_action_as_obs=True
TrainLoop.state_obs_selection=(@HafnerObservations(),)

# reward
HHHEnv.reward_calc=@HafnerRewardCalc()

# basic params
get_train_loop.alg_name='dqn'
TrainLoop.root_dir='/srv/bachmann/data/hafner'
TrainLoop.eval_interval = 300
TrainLoop.num_eval_episodes = 6
TrainLoop.log_interval = 200