# The config file for the DQN agent.
# Start DQN training with "cd <project-root> && python -m training.runner --gin_file=data/configs/dqn.gin"

##### Parameters that need to be changed to reproduce the results #####

# select scenario (@S1=S1, @S2=S2, @S3=S3)
DistributionTrace.traffic_trace_construct = @S3

# action space (@DqnMinPrefLenActionSpace() for DQN-L, @DqnRejectionActionSpace() for DQN-pthresh)
TrainLoop.action_space_selection=@DqnMinPrefLenActionSpace()

# was only enabled for scenario S3 (False to disable)
DQNWrapAgent.batch_norm=True

# the data will be stored at a sub-directory created at this path
TrainLoop.root_dir='/srv/bachmann/data/dqn'

# enables collection of raw trace capture (numpy arrays of generated traffic and rules)
# creates vast amounts of data!
TrainLoop.collect_raw = True



##### Parameters that remained unchanged across the complete evaluation #####


get_train_loop.alg_name='dqn'
TrainLoop.eval_interval = 3000
TrainLoop.num_eval_episodes = 9
TrainLoop.log_interval = 600
S1.num_benign=500
S1.num_attack=300
Loop.action_interval=10
Loop.sampling_rate=0.25
RulePerformanceTable.metric='fpr'
RulePerformanceTable.use_cache=True
RulePerformanceTable.cache_class=@PerformanceTrackingWorstOffenderCache
PerformanceTrackingWorstOffenderCache.metric='fpr'
PerformanceTrackingWorstOffenderCache.capacity='inf'

# state
TrainLoop.use_prev_action_as_obs=False
TrainLoop.state_obs_selection=(@TrafficSituation(),)

# reward
HHHEnv.reward_calc=@MultiplicativeRewardThesis()
MultiplicativeReward.precision_weight=0
MultiplicativeReward.recall_weight=1.5
MultiplicativeReward.fpr_weight=3
MultiplicativeReward.bl_weight=0.26

# generic params
DQNWrapAgent.lr=5e-5
DQNWrapAgent.lr_decay_rate=0.96
DQNWrapAgent.lr_decay_steps=1000
DQNWrapAgent.eps_greedy=1.0
DQNWrapAgent.eps_greedy_end=0.0
DQNWrapAgent.eps_greedy_steps=75000
TrainLoop.gamma=0
TrainLoop.num_iterations = 150000
TrainLoop.batch_size = 64
TrainLoop.replay_buf_size = 150000
TrainLoop.initial_collect_steps = 1200
TrainLoop.log_interval = 600

# the employed NN
DQNWrapAgent.use_rnn=True
TrainLoop.train_sequence_length=10
DQNWrapAgent.rnn_input_layers=(200, 200)
DQNWrapAgent.rnn_lstm_size=(128, 128, 128)
DQNWrapAgent.rnn_output_layers=(200,)

# CNN parameters
include 'data/configs/cnn.gin'
DQNWrapAgent.cnn_spec=%cnn_128_multi
DQNWrapAgent.cnn_act_func=@tf.keras.activations.relu
