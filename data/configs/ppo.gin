# basic params
get_train_loop.alg_name='ppo'
TrainLoop.root_dir='/home/bachmann/test-pycharm/data'
TrainLoop.eval_interval = 6000
TrainLoop.num_eval_episodes = 5
TrainLoop.log_interval = 600

# debug
TrainLoop.collect_raw = False # TODO warn
PPOWrapAgent.sum_grad_vars=False # TODO warn

# state
TrainLoop.use_prev_action_as_obs=True
TrainLoop.state_obs_selection=(@BaseObservations(), @MinMaxBlockedAddress(), @DistVol())#, @FalsePositiveRate())

# action
TrainLoop.actionset_selection=@TupleActionSet()

# reward
HHHEnv.reward_calc=@DefaultRewardCalc()

# trace settings
DistributionTrace.traffic_trace = @T3()
Loop.action_interval=10

# generic params
PPOWrapAgent.lr=1e-5
TrainLoop.gamma=0
PPOWrapAgent.use_gae=True
PPOWrapAgent.gae_lambda=0.95
TrainLoop.num_iterations = 100000
PPOWrapAgent.gradient_clip=None

# PPO-specific params
PPOWrapAgent.entropy_regularization=0.01
PPOWrapAgent.num_epochs=10
PPOWrapAgent.importance_ratio_clipping=0.1
PpoTrainLoop.num_parallel_envs = 1 # N
PpoTrainLoop.collect_steps_per_iteration_per_env = 5 # T

# act func for RNN/FFNN
PPOWrapAgent.actor_act_func=@tf.keras.activations.tanh
PPOWrapAgent.value_act_func=@tf.keras.activations.tanh

# RNN
#PPOWrapAgent.use_actor_rnn=True
#PPOWrapAgent.act_rnn_in_layers=(64,)
#PPOWrapAgent.act_rnn_lstm=(64,)
#PPOWrapAgent.act_rnn_out_layers=(64,)
#PPOWrapAgent.use_value_rnn=True
#PPOWrapAgent.val_rnn_in_layers=(64,)
#PPOWrapAgent.val_rnn_lstm=(64,)
#PPOWrapAgent.val_rnn_out_layers=(64,)

# FFNN
PPOWrapAgent.actor_layers=(64,64)
PPOWrapAgent.value_layers=(64,64)

# CNN parameters
include '/home/bachmann/test-pycharm/data/configs/cnn.gin'
PPOWrapAgent.cnn_act_func=@tf.keras.activations.relu
PPOWrapAgent.cnn_spec=%cnn_256
