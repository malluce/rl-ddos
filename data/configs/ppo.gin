# basic params
get_train_loop.alg_name='ppo'
TrainLoop.root_dir='/home/bachmann/test-pycharm/data'
TrainLoop.eval_interval = 6000
TrainLoop.num_eval_episodes = 5
TrainLoop.log_interval = 600

# debug
TrainLoop.collect_raw = False # TODO warn
PPOWrapAgent.sum_grad_vars=False # TODO warn

# state
TrainLoop.use_prev_action_as_obs=True
TrainLoop.state_obs_selection=(@BaseObservations(), @MinMaxBlockedAddress(), @DistVol(), @FalsePositiveRate())

# action
TrainLoop.actionset_selection=@TupleActionSet()

# reward
HHHEnv.reward_calc=@DefaultRewardCalc()

# trace settings
DistributionTrace.traffic_trace = @T4()
Loop.action_interval=10

# generic params
TrainLoop.gamma=0
PPOWrapAgent.use_gae=True
PPOWrapAgent.gae_lambda=0.95
TrainLoop.num_iterations = 300000
PPOWrapAgent.gradient_clip=0.5 # TODO disable again, only for RNN test

# PPO-specific params
PPOWrapAgent.lr=1e-6
PPOWrapAgent.num_epochs=10
PPOWrapAgent.importance_ratio_clipping=0.1
PpoTrainLoop.num_parallel_envs = 1 # N
PpoTrainLoop.collect_steps_per_iteration_per_env = 5 # T

# act func for RNN/FFNN
actor_act_func=@tf.keras.activations.relu
value_act_func=@tf.keras.activations.relu

# RNN
PPOWrapAgent.use_actor_rnn=True
PPOWrapAgent.act_rnn_in_layers=(64,)
PPOWrapAgent.act_rnn_lstm=(64,)
PPOWrapAgent.act_rnn_out_layers=(64,)
PPOWrapAgent.use_value_rnn=True
PPOWrapAgent.val_rnn_in_layers=(64,)
PPOWrapAgent.val_rnn_lstm=(64,)
PPOWrapAgent.val_rnn_out_layers=(64,)

# FFNN
#PPOWrapAgent.actor_layers=(64,64) # (200, 100)
#PPOWrapAgent.value_layers=(64,64) # (200, 100)

# CNN parameters see ppo-cnn.gin
