# The config file for the PPO agent.
# Start PPO training with "cd <project-root> && python -m training.runner --gin_file=<project-root>/data/configs/ppo.gin"

##### Parameters that need to be changed to reproduce the results #####

# select scenario (@S1=S1, @S2=S2, @S3=S3)
DistributionTrace.traffic_trace_construct = @S1

# the data will be stored at a sub-directory created at this path
TrainLoop.root_dir='/srv/bachmann/data/ppo'

# enables collection of raw trace capture (numpy arrays of generated traffic and rules) and debug info
# creates vast amounts of data!
TrainLoop.collect_raw = True
PPOWrapAgent.sum_grad_vars=False



##### Parameters that remained unchanged across the complete evaluation #####


get_train_loop.alg_name='ppo'
TrainLoop.eval_interval = 3000
TrainLoop.num_eval_episodes = 9
TrainLoop.log_interval = 600
S1.num_benign=500
S1.num_attack=300
Loop.action_interval=10
Loop.sampling_rate=0.25
RulePerformanceTable.metric='fpr'
RulePerformanceTable.use_cache=True
RulePerformanceTable.cache_class=@PerformanceTrackingWorstOffenderCache
PerformanceTrackingWorstOffenderCache.metric='fpr'
PerformanceTrackingWorstOffenderCache.capacity='inf'

# state
TrainLoop.use_prev_action_as_obs=False
TrainLoop.state_obs_selection=(@TrafficSituation(),)

# action
TrainLoop.action_space_selection=@ExponentialContinuousRejectionActionSpace()

# reward
HHHEnv.reward_calc=@MultiplicativeRewardThesis()
MultiplicativeReward.precision_weight=0
MultiplicativeReward.recall_weight=1.5
MultiplicativeReward.fpr_weight=3
MultiplicativeReward.bl_weight=0.26

# generic params
PPOWrapAgent.lr=2.5e-5
TrainLoop.gamma=0
PPOWrapAgent.use_gae=True
PPOWrapAgent.gae_lambda=0.95
TrainLoop.num_iterations = 100000
PPOWrapAgent.gradient_clip=None

# PPO-specific params
_normal_projection_net.init_action_stddev=0.35
PPOWrapAgent.entropy_regularization=0
PPOWrapAgent.num_epochs=5
PPOWrapAgent.importance_ratio_clipping=0.1
PpoTrainLoop.num_parallel_envs = 1 # N
PpoTrainLoop.collect_steps_per_iteration_per_env = 5 # T

# act func for FFNN
PPOWrapAgent.actor_act_func=@tf.keras.activations.tanh
PPOWrapAgent.value_act_func=@tf.keras.activations.tanh

# FFNN
PPOWrapAgent.batch_norm=False
PPOWrapAgent.actor_layers=(64,64)
PPOWrapAgent.value_layers=(64,64)

# CNN parameters
include 'cnn.gin'
PPOWrapAgent.cnn_act_func=@tf.keras.activations.relu
PPOWrapAgent.cnn_spec=%cnn_128_multi
